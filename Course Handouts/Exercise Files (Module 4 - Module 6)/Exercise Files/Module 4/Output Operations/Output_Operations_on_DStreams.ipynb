{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgcUHp1Cp54p"
      },
      "source": [
        "# Output Operations on DStreams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWcEOnoRp54w"
      },
      "source": [
        "Output operations allow DStream’s data to be pushed out to external systems like a database or a file systems. Since the output operations actually allow the transformed data to be consumed by external systems, they trigger the actual execution of all the DStream transformations (similar to actions for RDDs). Currently, the following output operations are defined:\n",
        "\n",
        "| Output Operation        | Meaning           |\n",
        "|-------------:|:------------- |\n",
        "| **pprint**()      | Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging. |\n",
        "| **saveAsTextFiles**(prefix, [suffix])     | Save this DStream's contents as text files. The file name at each batch interval is generated based on prefix and suffix: \"prefix-TIME_IN_MS[.suffix]\". |\n",
        "| **foreachRDD**(func) | The most generic output operator that applies a function, func, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function func is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs. |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwHl12bwp54x"
      },
      "source": [
        "In the Java and Scala APIs for Spark, there are also the `saveAsObjectFiles` and `saveAsHadoopFiles`. Unfortunately, these are not available in the Python API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8JA9MYFp54y"
      },
      "source": [
        "### Demo\n",
        "The following is a demonstration of how to output RDDs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMr8U0oJp54z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e7cae758-e7bf-4c21-d9cb-71c3b6206b4c"
      },
      "source": [
        "\"\"\"\n",
        "import findspark\n",
        "# TODO: your path will likely not have 'matthew' in it. Change it to reflect your path.\n",
        "findspark.init('/home/siddharth/spark-2.1.0-bin-hadoop2.7')\n",
        "\"\"\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport findspark\\n# TODO: your path will likely not have 'matthew' in it. Change it to reflect your path.\\nfindspark.init('/home/siddharth/spark-2.1.0-bin-hadoop2.7')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMGCo6EirIe4",
        "outputId": "92d64152-e3c9-42a6-c2c2-136a6178dcf2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=8fb607022efd9807975ba65861c04a0cc89486da1198d14c2ab8cc372f2c3599\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e5Ono5Dp541"
      },
      "source": [
        "import sys\n",
        "import pyspark\n",
        "import pyspark.streaming\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W32ZG_I5p541"
      },
      "source": [
        "sc = SparkContext(master=\"local[2]\", appName=\"WordcountToTextFiles\")\n",
        "ssc = StreamingContext(sc, 5)\n",
        "    \n",
        "lines = ssc.textFileStream('DrSeuss.text')\n",
        "\n",
        "def linesplit(line):\n",
        "    return line.split(\" \")\n",
        "\n",
        "counts = lines.flatMap(linesplit).map(lambda x: (x, 1)).reduceByKey(lambda a, b: a+b)\n",
        "counts.saveAsTextFiles(\"output/Counts\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ddlu7J2p542"
      },
      "source": [
        "ssc.start()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQKCz_vVp543"
      },
      "source": [
        "ssc.stop(stopSparkContext=True, stopGraceFully=True)#--files are generated and saved insid output/Counts"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6i0Tgycp544"
      },
      "source": [
        "## References\n",
        "1. https://spark.apache.org/docs/latest/streaming-programming-guide.html#output-operations-on-dstreams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7ToHU4Rp544"
      },
      "source": [
        " "
      ]
    }
  ]
}